Project Name - MapReduce Analysis on Amazon Cell Phones and Accessories Data

Project Guide - Prof. Yusuf Ozbek

Project Description - Perform data analysis over Amazon Cell Phones and Accessories Data using different MapReduce patterns. 

Dataset Link - http://jmcauley.ucsd.edu/data/amazon/links.html

Dataset Description - Data contains two files, Reviews.json and Metadata.json. Data used in the project is deduplicated.
1.Reviews - Contains reviews for each product from different users. Fields in the dataset:
	Metaprivate String reviewerID;
    private String asin;
    private String reviewerName;
    private String reviewText;
    private String overall;                         //This is the rating stars given by user.
    private String summary;
    private String unixReviewTime;
    private String reviewTime;
    private List<String> helpful;

2.Metadata - Contains metada about the product. Fields in the dataset:
	private String asin;
    private String title;
    private String price;
    private String imUrl;
    private RelatedProducts related;
    private Map<String, String> salesRank;
    private String brand;
    private List<List<String>> categories;

Technologies Used:
1. Hadoop 2.8.0
2. Spring Boot
3. Mahout
4. Pig
5. Redis
6. Sentiment API

Analysis Performed - 
1.Binning Pattern
	Binning pattern is used for organizing metadata into price beans. These bins are:
	 <$2, <$5, <$10, <$15 and >$15.
	 There are three input splits for each bin. 
Code - 
a.Binning Project.class
package BinningProject;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

import java.io.IOException;
import java.util.logging.Level;
import java.util.logging.Logger;

public class BinningProject {

    public static void main(String args[]) {
        Configuration configuration = new Configuration();
        try {
            Job job = Job.getInstance(configuration, "Price wise binning job");
            job.setJarByClass(BinningProject.class);

            job.setMapperClass(RatingBinningMapper.class);

            job.setNumReduceTasks(0);

            MultipleOutputs.addNamedOutput(job, "bins",
                    TextOutputFormat.class,
                    Text.class,
                    NullWritable.class);

            MultipleOutputs.setCountersEnabled(job, true);

            job.setOutputKeyClass(Text.class);
            job.setOutputValueClass(NullWritable.class);

            FileInputFormat.addInputPath(job, new Path(args[0]));
            FileOutputFormat.setOutputPath(job, new Path(args[1]));
            System.exit(job.waitForCompletion(true) ? 0 : 1);

        } catch (IOException | ClassNotFoundException ex) {
            Logger.getLogger(BinningProject.class.getName()).log(Level.SEVERE, ex.getMessage(), ex);
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}

b.RatingBinningMapper.class
package BinningProject;

import BinningProject.pojo.Metadata;
import com.google.gson.Gson;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.output.MultipleOutputs;

import java.io.IOException;

/**
 * Created by ajinkya on 4/21/17.
 */
public class RatingBinningMapper extends Mapper<Object, Text, Text, NullWritable> {
    private Gson gson;
    private MultipleOutputs<Text, NullWritable> multipleOutputs;

    @Override
    protected void setup(Context context) throws IOException, InterruptedException {
        multipleOutputs = new MultipleOutputs<>(context);
    }

    @Override
    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        gson = new Gson();
        Metadata metadata = gson.fromJson(value.toString(), Metadata.class);
        Double price = 0.00;
        if (metadata.getPrice() != null) {
            price = Double.parseDouble(metadata.getPrice());
        }
        if (price > Double.parseDouble("0")) {
            if (price <= Double.parseDouble("2.00")) {
                multipleOutputs.write("bins", value, NullWritable.get(), "less than $2");
            } else if (price <= Double.parseDouble("5.00")) {
                multipleOutputs.write("bins", value, NullWritable.get(), "less than $5");
            } else if (price <= Double.parseDouble("10.00")) {
                multipleOutputs.write("bins", value, NullWritable.get(), "less than $10");
            } else if (price <= Double.parseDouble("15.00")) {
                multipleOutputs.write("bins", value, NullWritable.get(), "less than $15");
            } else {
                multipleOutputs.write("bins", value, NullWritable.get(), "greater than $15");
            }
        }
    }

    @Override
    protected void cleanup(Context context) throws IOException, InterruptedException {
        multipleOutputs.close();
    }
}


2.Partitioning Pattern - RatingWisePartitioning
	Used partitioning pattern for clusterning reviews into separate files as per rating. 
	Partioner is used.
Code - 
a.MapperByRating.class
package RatingWisePartitioning;

import RatingWisePartitioning.pojo.Review;
import com.google.gson.Gson;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class MapperByRating extends Mapper<Object,Text,DoubleWritable,Text> {
    private Gson gson;
    @Override
    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        gson = new Gson();
        Review emitReview = gson.fromJson(value.toString(), Review.class);
        Double rating = Double.parseDouble(emitReview.getOverall());
        context.write(new DoubleWritable(rating), value);
    }
}

b.PartitionRating.class
package RatingWisePartitioning;

import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * Created by ajinkya on 4/24/17.
 */
public class PartitionRating extends Partitioner<DoubleWritable, Text> implements Configurable {
    private Configuration configuration;

    @Override
    public Configuration getConf() {
        return configuration;
    }

    @Override
    public void setConf(Configuration configuration) {
        this.configuration = configuration;
    }

    @Override
    public int getPartition(DoubleWritable doubleWritable, Text text, int i) {
        return (int) doubleWritable.get() - 1;
    }
}

c.RatingWisePartitioning.class
package RatingWisePartitioning;

import RatingWisePartitioning.pojo.Review;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class RatingWisePartitioning {
    public static void main(String args[]) {
        Configuration configuration = new Configuration();

        try {
            Job ratingPartitioningJob = Job.getInstance(configuration, "Partitioning by Rating");
            ratingPartitioningJob.setJarByClass(RatingWisePartitioning.class);

            ratingPartitioningJob.setMapperClass(MapperByRating.class);
            ratingPartitioningJob.setMapOutputKeyClass(DoubleWritable.class);
            ratingPartitioningJob.setMapOutputValueClass(Text.class);

            ratingPartitioningJob.setReducerClass(ReducerByRating.class);
            ratingPartitioningJob.setOutputKeyClass(DoubleWritable.class);
            ratingPartitioningJob.setOutputValueClass(Text.class);
            ratingPartitioningJob.setNumReduceTasks(5);

            ratingPartitioningJob.setPartitionerClass(PartitionRating.class);
            FileInputFormat.addInputPath(ratingPartitioningJob, new Path(args[0]));
            FileOutputFormat.setOutputPath(ratingPartitioningJob, new Path(args[1]));
            System.exit(ratingPartitioningJob.waitForCompletion(true) ? 0 : 1);

        } catch (IOException | ClassNotFoundException | InterruptedException e) {
            e.printStackTrace();
        }
    }
}

d.ReducerByRating.class
package RatingWisePartitioning;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class ReducerByRating extends Reducer<DoubleWritable, Text, Text, NullWritable> {
    @Override
    protected void reduce(DoubleWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        for (Text value : values) {
            context.write(value, NullWritable.get());
        }
    }
}


3.Summarization Pattern - ReviewSummarization
	Used summarization pattern for calculating average, max and min rating for products. Used custom writable class for output. 
	File is used for pig joins.
Code - 
a.MapperByRating
package RatingWisePartitioning;

import RatingWisePartitioning.pojo.Review;
import com.google.gson.Gson;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Mapper;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class MapperByRating extends Mapper<Object,Text,DoubleWritable,Text> {
    private Gson gson;
    @Override
    protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
        gson = new Gson();
        Review emitReview = gson.fromJson(value.toString(), Review.class);
        Double rating = Double.parseDouble(emitReview.getOverall());
        context.write(new DoubleWritable(rating), value);
    }
}

b.PartitionRating
package RatingWisePartitioning;

import org.apache.hadoop.conf.Configurable;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Partitioner;

/**
 * Created by ajinkya on 4/24/17.
 */
public class PartitionRating extends Partitioner<DoubleWritable, Text> implements Configurable {
    private Configuration configuration;

    @Override
    public Configuration getConf() {
        return configuration;
    }

    @Override
    public void setConf(Configuration configuration) {
        this.configuration = configuration;
    }

    @Override
    public int getPartition(DoubleWritable doubleWritable, Text text, int i) {
        return (int) doubleWritable.get() - 1;
    }
}

c.RatingWisePartitioning
package RatingWisePartitioning;

import RatingWisePartitioning.pojo.Review;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class RatingWisePartitioning {
    public static void main(String args[]) {
        Configuration configuration = new Configuration();

        try {
            Job ratingPartitioningJob = Job.getInstance(configuration, "Partitioning by Rating");
            ratingPartitioningJob.setJarByClass(RatingWisePartitioning.class);

            ratingPartitioningJob.setMapperClass(MapperByRating.class);
            ratingPartitioningJob.setMapOutputKeyClass(DoubleWritable.class);
            ratingPartitioningJob.setMapOutputValueClass(Text.class);

            ratingPartitioningJob.setReducerClass(ReducerByRating.class);
            ratingPartitioningJob.setOutputKeyClass(DoubleWritable.class);
            ratingPartitioningJob.setOutputValueClass(Text.class);
            ratingPartitioningJob.setNumReduceTasks(5);

            ratingPartitioningJob.setPartitionerClass(PartitionRating.class);
            FileInputFormat.addInputPath(ratingPartitioningJob, new Path(args[0]));
            FileOutputFormat.setOutputPath(ratingPartitioningJob, new Path(args[1]));
            System.exit(ratingPartitioningJob.waitForCompletion(true) ? 0 : 1);

        } catch (IOException | ClassNotFoundException | InterruptedException e) {
            e.printStackTrace();
        }
    }
}

d.ReducerByRating
package RatingWisePartitioning;

import org.apache.hadoop.io.DoubleWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

/**
 * Created by ajinkya on 4/24/17.
 */
public class ReducerByRating extends Reducer<DoubleWritable, Text, Text, NullWritable> {
    @Override
    protected void reduce(DoubleWritable key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
        for (Text value : values) {
            context.write(value, NullWritable.get());
        }
    }
}

	
4.Top Distinct Categories - TopDistictCetegories
	Used job chaining. First job gets distinct categories from metadata of products. Second job gives the top ten category results. Used TreeMap to sort keys and get top ten.
5.Top Category Partititons - FindingTopRatedProducts
	Based on the top ten categories from the above job, we added these top categories into distrbuted cache from local file. Then partitioned metadata according the categories. 
	This output is later used in pig join, to find top 10 products in each category.
6.PIG 
	Get top ten products for each top category for faster retrieval from huge category parititons. 
	Used the data from summarization job and top category data to perform join. 
	Result is top rated products in each top rated category.
	
Code - 
a.join.pig
raw = LOAD '/project/out_summarization_reviews/part-r-00000' USING PigStorage(',') AS (asin:chararray, average:float);

--raw = LOAD '/Users/ajinkya/Downloads/pigtmp/input_top10/top-10' USING PigStorage(',') AS (asin:chararray, average:float);
----------------------------	FIRST JOIN -------------------------------------------- 

toJoin = LOAD '/project/out_top10_by_category/final/part-r-00008' USING PigStorage('\t') AS (category:chararray, asin:chararray);

--toJoin = LOAD '/Users/ajinkya/Downloads/pigtmp/input_top10/part-r/part-r-00008' USING PigStorage('\t') AS (category:chararray, asin:chararray);

joinedList = JOIN raw BY asin, toJoin BY asin;
ordered_list = ORDER joinedList BY average DESC;
top_10_items_first = LIMIT ordered_list 10;

STORE top_10_items_first INTO '/pig_temp/part-8-out' USING PigStorage();

--STORE top_10_items_first INTO '/Users/ajinkya/Downloads/pigtmp/top_10_out' USING PigStorage();
--STORE raw INTO '/pig_join' USING PigStorage();

b.category_unique.pig
raw = LOAD 'metadata.json'
 	USING JsonLoader('asin:chararray,
                      categories:{
					  	(category:chararray)
					  }');
STORE raw 
    INTO 'myCategories.json' 
    USING JsonStorage();
	
c.top_ten.pig
raw = LOAD 'input_top10/top-10' USING PigStorage(',') AS (asin:chararray, average:float);

ordered_list = ORDER raw BY average DESC;

top_10_items = LIMIT ordered_list 10;

--DUMP top_10_items

STORE top_10_items INTO 'out_top_10' USING PigStorage();

7.Simple Randon Sampling - 
	Created random data for sentimental analysis. 
8.Sentimental Analysis - 
	performed sentimental analysis on customer reviews on the sampled data from the above job. 
	IBM watson natural language understanding API provides various entities for judging the sentiments of a text. 
	The api analyses the text input based on keywords cache. It also outputs the emotion of sentiment. Finally calculating the sentiment score. 
9.Mahout Recommendation - 
	Mahout user based recommendation to search a user and get products recommended for him according to user similarity. 
		 